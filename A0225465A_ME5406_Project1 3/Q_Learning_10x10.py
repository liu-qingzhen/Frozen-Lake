# Import Numpy, modified gym frozen-lake environment, matplotlib

import numpy as np
from me5406_env import FrozenLakeEnv
from matplotlib import pyplot as plt
import os
import shutil

# parameters to tune
# episode; training episode
# iteration: the max allowed step length in each episode
# test_episode, test_iter: episode and max allowed step length for evaluation
# mean_reward_calc_epi: Every such episodes, calculate mean reward
# map_size: the size of the map

Epsilon_Decay = True
episode = 200
iteration = 1000
learning_rate = 0.5
gamma = 0.9
epsilon = 0.4
test_episode = 100
test_iter = 200
mean_reward_calc_epi = 10
map_size = 10

# path to save training records later

game_name = 'Figs_Q_learning_%dx%d_Gamma=%.2f_eps=%.2f_episode=%d'%(map_size,map_size,gamma,epsilon,episode)
path = os.getcwd()+'/Training_Records/%s'%(game_name)


# To visualize the map and best policy with dictionary

mapping_policy = {0: '‚á¶', 1: '‚á©', 2: '‚á®', 3:'‚áß'}
mapping_map =  {'S': 'üèÇ', 'F': '‚ùÑÔ∏è', 'H': 'üï≥Ô∏èÔ∏è', 'G':'üèÅ'}


# Generate the 4x4 map of the project if map_size is 4
# else, Generate random map of map_size

if map_size ==  10:
	env = FrozenLakeEnv(map_size=10)
elif map_size == 4:
	env = FrozenLakeEnv(map_name='4x4')
elif map_size == 5:
	env = FrozenLakeEnv(map_name='5x5')
else: env = FrozenLakeEnv(map_size=map_size)


# Create Q-table

q_table = np.zeros((env.observation_space.n,env.action_space.n))


# Epsilon-Greedy method to generate a new action from state s
# a is the action which has the largest value of state s in Q-table

def epsilon_greedy(action,epsilon=0.3):
	p = np.random.random()
	if p < epsilon:
		return np.random.choice(env.action_space.n)
	else:
		return action


# Return the index of max numbers in an 1-d array
# Because np.argmax always return the index of the first largest value
# in an array, which will not return a action which has the same largest
# value in Q-table. So I rewrite a function to get the random choice of
# state-action when they have the same value in Q-table.

def maxindex(arg):
	index = []
	max = float('-inf')
	while True:
		for i in range (len(arg)):
			if arg[i] > max:
				index=[i]
				max = arg[i]
			elif arg[i] == max:
				index.append(i)
		break

	return index


# rALL: total rewards, y: total reward at each episode z: reward of each episode l: length of each episode

rAll=0
x=[]
y=[]
z=[]
l=[]

# Accumulated reward of certain episodes
# For computation of mean_reward

periodic_reward = 0

# start training

for epi in range(episode):
	if (epi % mean_reward_calc_epi == 0):
		print('Episode: %d Mean_reward: %.3f' % (epi,periodic_reward / mean_reward_calc_epi))
		periodic_reward=0

	# reset at every new episode
	s = env.reset()

	# set epsilon
	if Epsilon_Decay == True:
		eps = max(0,epsilon-epi/episode)
	else: eps = epsilon

	for iter in range(iteration):

		# choose action based on epsilon-greedy
		action = np.random.choice(maxindex(q_table[s,:]))
		a = epsilon_greedy(action,epsilon=eps)

		# take an action and update Q-table
		state_new,reward,done,_ = env.step(a)
		oldq = q_table[s,a]
		q_table[s,a] = q_table[s,a] + learning_rate*(reward + gamma*np.max(q_table[state_new,:]) - q_table[s,a])
		s = state_new

		# add data to arrays if the episode in finished
		if done:
			if reward == 1:
				periodic_reward += reward
			rAll=rAll+reward
			x.append(epi)
			y.append(rAll)
			z.append(reward)
			l.append(iter)

			break

# print Q-table

print(np.around(q_table,4))


# evaluation   same as training but not update Q-table

rewardtest = 0
for i in range(test_episode):
	s = env.reset()
	for iter in range(test_iter):
		a = np.argmax(q_table[s,:])
		state_new,reward,done,_ = env.step(a)
		if i == test_episode-1:
			env.render()
		s = state_new
		if done:
			if reward == 1:
				rewardtest +=1
			break

# print Q-table

print('\nPrint_Q_table\n')
print(np.around(q_table,4))
print('\n')


# print best policy

print('\nBest_Policy:\n')
best_policy = np.argmax(q_table,axis = 1)
best_policy.reshape(map_size,map_size)
best_policy = [mapping_policy[x] if x in mapping_policy else x for x in best_policy]
best_policy = np.array(best_policy)
print(best_policy.reshape(map_size,map_size))
print('\n')

# Print map generated by the environment

print("MAP_GENERATED:\n")
map = env.desc1
map = desc = [[mapping_map[c] for c in line] for line in map]
map = np.array(map).reshape(map_size,map_size)
print(map)
print('\n')

# Print success rate

print('success_rate=%.3f'%(rewardtest*1.0/test_episode))

# if training was successful, then create file to save images of this training

success = False

if rewardtest == test_episode:
	success = True

if success:
	if os.path.exists(path):
		shutil.rmtree(path)
	os.mkdir(path)


# Print total reward

plt.xlabel("Episode(epsilon=%.2f, Epsilon_decay=True)"%(epsilon))
plt.ylabel("Total_Reward")
plt.plot(x,y)
if success:
	plt.savefig('%s/Total_Reward_Episode(epsilon=%.2f, Epsilon_decay=True).jpg'%(path,epsilon))
plt.show()

# Print reward of each episode

plt.xlabel("Episode(epsilon=%.2f, Epsilon_decay=True)"%(epsilon))
plt.ylabel("Reward_of_Each_episode")
plt.plot(x,z)
if success:
	plt.savefig('%s/Reward_of_Each_episode_Episode(epsilon=%.2f, Epsilon_decay=True).jpg'%(path,epsilon))
plt.show()

# Print step length of each episode

plt.xlabel("Episode(epsilon=%.2f, Epsilon_decay=True)"%(epsilon))
plt.ylabel("length of each episode")
plt.plot(x,l)
if success:
	plt.savefig('%s/Step_Length_(epsilon=%.2f, Epsilon_decay=True).jpg'%(path,epsilon))
plt.show()


